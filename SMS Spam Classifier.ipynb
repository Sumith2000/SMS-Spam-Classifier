{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954189a1-d89b-4ade-9292-206f1a6cf8f2",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d453f3-97ba-48be-b3db-ea9cd8d344e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ecb9d-ea8f-4dd0-8237-906e4d7e5209",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ec2123-9829-45b5-b53c-875d6ef7add1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam text message.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06e434a7-b31c-4fb2-9881-8f2c2a3af74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e86df-9cd0-434b-ae12-07855c349d18",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b306e21-8daa-40c2-830d-1699b5bca16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39f16a31-0a57-4330-868d-23ce98ba28e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e2b5bc-be63-4ad9-8979-cc1474caecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df['Category']=le.fit_transform(df['Category'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54e2225f-7fee-46b8-a384-9ed4505ba9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                      Ok lar... Joking wif u oni...\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the columns because I feel fancy today\n",
    "df.rename(columns = {\"Category\":\"Target\", \"Message\":\"Text\"}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bdbb8-6ad9-4100-ae17-57b35bdaa04b",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0f32016-e865-4517-b3fd-3c98201e32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amith\\AppData\\Local\\Temp\\ipykernel_17152\\4200349490.py:2: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.pie(df['Target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cbcde9f-ffa6-4e2e-8d96-49847d041fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Data points')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first of all let us evaluate the target and find out if our data is imbalanced or not\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= df[\"Target\"])\n",
    "fg.set_title(\"Count Plot of Classes\", color=\"blue\")\n",
    "fg.set_xlabel(\"Classes\", color=\"#58508d\")\n",
    "fg.set_ylabel(\"Number of Data points\", color=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfe7d672-b90a-4d0f-a4ba-9b16d5c0b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88aab391-524a-43af-888f-b8d8bb9074b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts after cleaning:\u001b[0m\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "ok lar joking wif u oni\n",
      "free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry question std txt rate t c s apply over s\n",
      "u dun say so early hor u c already then say\n",
      "nah i don t think he goes to usf he lives around here though\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to clean up the text\n",
    "def Clean(Text):\n",
    "    sms = re.sub('[^a-zA-Z]', ' ', Text) \n",
    "    sms = sms.lower() \n",
    "    sms = sms.split()\n",
    "    sms = ' '.join(sms)\n",
    "    return sms\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(Clean)\n",
    "#Lets have a look at a sample of texts after cleaning\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after cleaning:\\033[0m\",*df[\"Clean_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94b34581-8196-4c01-9efe-c7cc60832f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts after Tokenizing:\u001b[0m\n",
      "['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
      "['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n",
      "['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n",
      "['nah', 'i', 'don', 't', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']\n"
     ]
    }
   ],
   "source": [
    "df[\"Tokenize_Text\"]=df.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), axis=1)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after Tokenizing:\\033[0m\",*df[\"Tokenize_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abc40476-2bd0-4778-8686-a3767e6157ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts after removing the stopwords:\u001b[0m\n",
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n",
      "['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']\n",
      "['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though']\n"
     ]
    }
   ],
   "source": [
    "# Removing the stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "df[\"Nostopword_Text\"] = df[\"Tokenize_Text\"].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after removing the stopwords:\\033[0m\",*df[\"Nostopword_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3021b662-6acc-468d-b943-95a19392c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e34c4563-c50c-4027-be1c-f64a58cfa9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts after lemitization:\u001b[0m\n",
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'get', 'amore', 'wat']\n",
      "['ok', 'lar', 'joke', 'wif', 'u', 'oni']\n",
      "['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']\n",
      "['nah', 'think', 'go', 'usf', 'live', 'around', 'though']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    #word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "df[\"Lemmatized_Text\"] = df[\"Nostopword_Text\"].apply(lemmatize_word)\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after lemitization:\\033[0m\",*df[\"Lemmatized_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e600e94-1ae3-4aa8-bbe2-2a6eda5e34eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 lines in corpus :\u001b[0m\n",
      "go jurong point crazy available bugis n great world la e buffet cine get amore wat\n",
      "ok lar joke wif u oni\n",
      "free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply\n",
      "u dun say early hor u c already say\n",
      "nah think go usf live around though\n"
     ]
    }
   ],
   "source": [
    "#Creating a corpus of text feature to encode further into vectorized form\n",
    "corpus= []\n",
    "for i in df[\"Lemmatized_Text\"]:\n",
    "    msg = ' '.join([row for row in i])\n",
    "    corpus.append(msg)\n",
    "\n",
    "corpus[:5]\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 lines in corpus :\\033[0m\",*corpus[:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f9bc588-3e4f-4008-83f4-1dd07189de7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing text data in to numbers.\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "#Let's have a look at our feature\n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "edec714d-21bb-4699-adec-a19f55d719c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encode the Target and use it as y\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Target\"] = label_encoder.fit_transform(df[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5cd3c64-8303-4a17-99da-7a0ba3bab659",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Target\"]\n",
    "# Splitting the testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ff6ab1c-1296-4d44-91c9-886d8e2a6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on the following classifiers\n",
    "classifiers = [MultinomialNB(),\n",
    "               RandomForestClassifier(),\n",
    "               KNeighborsClassifier(),\n",
    "               SVC()]\n",
    "for cls in classifiers:\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "# Dictionary of pipelines and model types for ease of reference\n",
    "pipe_dict = {0: \"NaiveBayes\", 1: \"RandomForest\", 2: \"KNeighbours\",3: \"SVC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "889179ab-3133-41b4-81d2-655d52eaf6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes: 0.964774 \n",
      "RandomForest: 0.976218 \n",
      "KNeighbours: 0.912497 \n",
      "SVC: 0.975768 \n"
     ]
    }
   ],
   "source": [
    "#Crossvalidation\n",
    "for i, model in enumerate(classifiers):\n",
    "    cv_score = cross_val_score(model, X_train,y_train,scoring=\"accuracy\", cv=10)\n",
    "    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ada8db0-33c0-4203-8888-f6bd6fd15046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# creating lists of varios scores\n",
    "precision =[]\n",
    "recall =[]\n",
    "f1_score = []\n",
    "trainset_accuracy = []\n",
    "testset_accuracy = []\n",
    "\n",
    "for i in classifiers:\n",
    "    pred_train = i.predict(X_train)\n",
    "    pred_test = i.predict(X_test)\n",
    "    prec = metrics.precision_score(y_test, pred_test)\n",
    "    recal = metrics.recall_score(y_test, pred_test)\n",
    "    f1_s = metrics.f1_score(y_test, pred_test)\n",
    "    train_accuracy = model.score(X_train,y_train)\n",
    "    test_accuracy = model.score(X_test,y_test)\n",
    "\n",
    "    #Appending scores\n",
    "    precision.append(prec)\n",
    "    recall.append(recal)\n",
    "    f1_score.append(f1_s)\n",
    "    trainset_accuracy.append(train_accuracy)\n",
    "    testset_accuracy.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "874ac384-4fd1-4a67-b0a3-fcafe4d3e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data of lists.\n",
    "data = {'Precision':precision,\n",
    "'Recall':recall,\n",
    "'F1score':f1_score,\n",
    "'Accuracy on Testset':testset_accuracy,\n",
    "'Accuracy on Trainset':trainset_accuracy}\n",
    "# Creates pandas DataFrame.\n",
    "Results = pd.DataFrame(data, index =[\"NaiveBayes\", \"RandomForest\", \"KNeighbours\",\"SVC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d097bc6-f494-4d61-b2d3-3cc91afe2a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_58a13_row0_col0, #T_58a13_row0_col1, #T_58a13_row0_col2, #T_58a13_row1_col0, #T_58a13_row1_col1, #T_58a13_row1_col2, #T_58a13_row2_col0, #T_58a13_row3_col1, #T_58a13_row3_col2 {\n",
       "  background-color: #e598d8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_58a13_row0_col3, #T_58a13_row0_col4, #T_58a13_row1_col3, #T_58a13_row1_col4, #T_58a13_row2_col1, #T_58a13_row2_col2, #T_58a13_row2_col3, #T_58a13_row2_col4, #T_58a13_row3_col0, #T_58a13_row3_col3, #T_58a13_row3_col4 {\n",
       "  background-color: #e2ccff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_58a13\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_58a13_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_58a13_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_58a13_level0_col2\" class=\"col_heading level0 col2\" >F1score</th>\n",
       "      <th id=\"T_58a13_level0_col3\" class=\"col_heading level0 col3\" >Accuracy on Testset</th>\n",
       "      <th id=\"T_58a13_level0_col4\" class=\"col_heading level0 col4\" >Accuracy on Trainset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_58a13_level0_row0\" class=\"row_heading level0 row0\" >NaiveBayes</th>\n",
       "      <td id=\"T_58a13_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_58a13_row0_col1\" class=\"data row0 col1\" >0.758389</td>\n",
       "      <td id=\"T_58a13_row0_col2\" class=\"data row0 col2\" >0.862595</td>\n",
       "      <td id=\"T_58a13_row0_col3\" class=\"data row0 col3\" >0.975785</td>\n",
       "      <td id=\"T_58a13_row0_col4\" class=\"data row0 col4\" >0.997756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58a13_level0_row1\" class=\"row_heading level0 row1\" >RandomForest</th>\n",
       "      <td id=\"T_58a13_row1_col0\" class=\"data row1 col0\" >1.000000</td>\n",
       "      <td id=\"T_58a13_row1_col1\" class=\"data row1 col1\" >0.832215</td>\n",
       "      <td id=\"T_58a13_row1_col2\" class=\"data row1 col2\" >0.908425</td>\n",
       "      <td id=\"T_58a13_row1_col3\" class=\"data row1 col3\" >0.975785</td>\n",
       "      <td id=\"T_58a13_row1_col4\" class=\"data row1 col4\" >0.997756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58a13_level0_row2\" class=\"row_heading level0 row2\" >KNeighbours</th>\n",
       "      <td id=\"T_58a13_row2_col0\" class=\"data row2 col0\" >1.000000</td>\n",
       "      <td id=\"T_58a13_row2_col1\" class=\"data row2 col1\" >0.429530</td>\n",
       "      <td id=\"T_58a13_row2_col2\" class=\"data row2 col2\" >0.600939</td>\n",
       "      <td id=\"T_58a13_row2_col3\" class=\"data row2 col3\" >0.975785</td>\n",
       "      <td id=\"T_58a13_row2_col4\" class=\"data row2 col4\" >0.997756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58a13_level0_row3\" class=\"row_heading level0 row3\" >SVC</th>\n",
       "      <td id=\"T_58a13_row3_col0\" class=\"data row3 col0\" >0.991935</td>\n",
       "      <td id=\"T_58a13_row3_col1\" class=\"data row3 col1\" >0.825503</td>\n",
       "      <td id=\"T_58a13_row3_col2\" class=\"data row3 col2\" >0.901099</td>\n",
       "      <td id=\"T_58a13_row3_col3\" class=\"data row3 col3\" >0.975785</td>\n",
       "      <td id=\"T_58a13_row3_col4\" class=\"data row3 col4\" >0.997756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1bcb364a390>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap2 = ListedColormap([\"#E2CCFF\",\"#E598D8\"])\n",
    "Results.style.background_gradient(cmap=cmap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51aaaa47-1584-454c-99d8-7885703afd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\amith\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (5.4.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (24.0)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (5.27.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (17.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\amith\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\amith\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amith\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b2ac6a83-cf97-4019-80f1-18e87515eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample SMS data (Replace this with your own dataset)\n",
    "sms_data = [\n",
    "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005\", 1),\n",
    "    (\"U dun say so early hor... U c already then say...\", 0),\n",
    "    (\"Nah I don't think he goes to usf, he lives around here though\", 0),\n",
    "    (\"FreeMsg Hey there darling it's been 3 week's now and no word back!\", 1),\n",
    "    (\"Even my brother is not like to speak with me. They treat me like aids patent.\", 0),\n",
    "    (\"WINNER!! As a valued network customer you have been selected to receive a $900 prize reward!\", 1)\n",
    "]\n",
    "\n",
    "# Separate the messages and labels\n",
    "X, y = zip(*sms_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the SMS data to TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Function to classify SMS\n",
    "def classify_sms():\n",
    "    sms_text = sms_entry.get()\n",
    "    if sms_text.strip() == \"\":\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter an SMS message.\")\n",
    "        return\n",
    "    \n",
    "    # Transform and predict\n",
    "    input_tfidf = vectorizer.transform([sms_text])\n",
    "    prediction = model.predict(input_tfidf)\n",
    "    \n",
    "    if prediction[0] == 1:\n",
    "        result_label.config(text=\"This SMS is classified as SPAM.\", fg=\"red\")\n",
    "    else:\n",
    "        result_label.config(text=\"This SMS is classified as HAM (Not Spam).\", fg=\"green\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"SMS Spam Classifier\")\n",
    "\n",
    "# Create and place the input field\n",
    "tk.Label(root, text=\"Enter SMS:\").pack(pady=5)\n",
    "sms_entry = tk.Entry(root, width=50)\n",
    "sms_entry.pack(pady=5)\n",
    "\n",
    "# Create and place the classify button\n",
    "classify_button = tk.Button(root, text=\"Classify\", command=classify_sms)\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "# Create and place the result label\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Arial\", 14))\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f96324-eb21-4df3-9a03-74c8ddd1fb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
